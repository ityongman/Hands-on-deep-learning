{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 张量 tensor --  n维数组，也称为张量（tensor）张量表示一个由数值组成的数组，这个数组可能有多个维度\n",
    "    - 具有一个轴的张量对应数学上的向量（vector）\n",
    "    - 具有两个轴的张量对应数学上的矩阵（matrix）\n",
    "    - 具有两个轴以上的张量没有特殊的数学名称\n",
    "- concatenate -- 多个张量连结在一起， 让它们端对端地叠起来形成一个更大的张量\n",
    "- norm 范数 -- 范数表示向量 或 矩阵的长度, L2范数是向量元素平方和的平方根\n",
    "    - 第一个性质, 如果按常数因子缩放向量的所有元素， 其范数也会按相同常数因子的绝对值缩放 f(ax)=|a|f(x)\n",
    "    - 第二个性质, 三角不等式 f(x+y) <= f(x)+f(y)\n",
    "    - 第三个性质, 简单地说范数必须是非负的 f(x) >= 0\n",
    "    - 最后一个性质, 要求范数最小为0，当且仅当向量全由0组成 f(x) == 0\n",
    "- integral calculus -- 积分, 逼近法就是积分(integral calculus)的起源\n",
    "- optimization -- 优化, 用模型拟合观测数据的过程\n",
    "- generalization -- 泛化, 数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型\n",
    "- 损失函数 -- 对于每个参数， 如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少\n",
    "    - 函数定义f'(x) = lim(f(x+h)-f(x))/h (h->0 无限趋近与0)\n",
    "    - 导数f'(x)可以解释为 f(x) 相对于 x 的瞬时(instantaneous)变化率\n",
    "- differentiable -- 可微, 如果f'(a)存在则表示f(x)在a点可微\n",
    "- computational graph -- 计算图\n",
    "- automatic differentiation -- 自动微分, 深度学习框架通过自动计算导数来加快求导\n",
    "- backpropagate -- 反向传播\n",
    "- dot -- 点积\n",
    "- 矩阵-向量积\n",
    "```text\n",
    "  矩阵向量积Ax是一个长度为m的列向量， 其第i个元素是点积 torch.dot(A(i), x),\n",
    "A = m * n 的矩阵,\n",
    "x = 1 * n,\n",
    "第i个元素是, A的第i行的 n 个列元素 和 x 进行点积的结果\n",
    "```\n",
    "- 矩阵乘法\n",
    "```text\n",
    "点积 -> 矩阵向量积 -> 矩阵-矩阵乘法\n",
    "矩阵-矩阵 乘法: AB, A=m*n, B=n*k  计算结果 C=m * k, C[i,j] = A[i,:] * B[:,j] 表示 A的第i行所有n个列元素 和 B的j列，所有的行元素继续点积的值\n",
    "C[i, :] 第i行所有元素 = A[i,:] * B[:, :] i行每列的元素 = A矩阵第i行的所有元素分别和b矩阵k列的值做 矩阵-向量积\n",
    "矩阵-矩阵乘法可以简单地称为矩阵乘法，不应与“Hadamard积”混淆\n",
    "- 矩阵乘法 torch.mm(A, B)\n",
    "- Hadamard积 A * B\n",
    "```\n",
    "- 导数、微分\n",
    "|  特性 |  导数   | 微分  |\n",
    "| ---- |  ----  | ----  |\n",
    "|  定义 | 变化率的极限值 | 变化量的主部 |\n",
    "|  符号 | dy/d(x) 或 f'(x)  | dy 或 df |\n",
    "|  几何意义 | 切线斜率  | 切线上的增量 |\n",
    "|  计算目标 | 求极限 lim Δy/Δx (Δx -> 0)  | 求线性近似 dy=f'(x)dx |\n",
    "\n",
    "- sampling -- 抽样 把从概率分布中抽取样本的过程称为抽样\n",
    "- distribution -- 分布 看作对事件的概率分配\n",
    "- multinomial distribution -- 多项式分布  将概率分配给一些离散选择的分布\n",
    "- probability -- 概率  可以被认为是将集合映射到真实值的函数\n",
    "- discrete -- 离散\n",
    "- continuous -- 连续\n",
    "- density -- 密度 看到某个数值的可能性\n",
    "- 随机变量\n",
    "- 联合概率 joint probability P(A=a, B=b)=P(A=a)P(B=b) 表示 a和b同时发生的概率\n",
    "- 条件概率 conditional probability P(A=a|B=b) = P(A=a, B=b)/P(B=b) 表示在b发生的情况下, a,b 同时发生的概率\n",
    "- 贝叶斯定理 Bayes定理 P(A|B) = P(B|A)P(A)/P(B)、注意: P(A, B) = P(B, A) 表示A和B同时发生的概率\n",
    "- regression -- 回归\n",
    "- analytical solution -- 解析解, 可以公式表达出来的解\n",
    "- gradient descent -- 梯度下降  这种方法几乎可以优化所有深度学习模型\n",
    "- loss function -- 损失函数  能够量化目标的实际值与预测值之间的差距, 数值越小表示损失越小，完美预测时的损失为0\n",
    "- minibatch stochastic gradient descent -- 小批量随机梯度下降, 在每次需要计算更新的时候随机抽取一小批样本\n",
    "- prediction(预测)、inference(推断) -- 给定特征 估计目标的过程\n",
    "- argmin -- 作用是找到使某个函数取得最小值的自变量值\n",
    "- Gradient Descent, GD -- 梯度下降 通过迭代调整参数来最小化目标函数(比如 损失函数)\n",
    "- 梯度, 将导数拓展到向量, 表示对向量求导\n",
    "- Dimensions -- 维数 0-d 0维 表示标量, 1-d 1维\n",
    "- axis -- 轴  axis = 0 表示行向量(实际处理数据是处理列, 表示压缩了行, 最后只有一行数据, 列的数量表示不变), axis = 1 表示列向量(实际处理数据是处理行, 表示压缩了列, 最后只有一列的数据, 行的数量不变)\n",
    "- cumsum 函数, 累加求和\n",
    "- ||x||^2 = xi^2 + x2^2 + ... + xn^2 = x^t x >= 0\n",
    "- 特殊矩阵\n",
    "    - 对称矩阵 -- $x_{i,j} = x_{j,i}$ 即 $A=A^T$\n",
    "    - 反对称矩阵 -- $x_{i,j} = -x_{j,i}$ 即 $A=-A^T$\n",
    "    - 正定矩阵 -- $\\left\\| x \\right\\|^2$ = $x^tx$ >= 0, 如果可以找到一个矩阵A, $x^tAx$ >= 0, 则称A为正定矩阵\n",
    "    - 正交矩阵 -- 向量之间两两正交且长度为1\n",
    "    - 逆矩阵 -- 如果 AB=BA=I, 则称B为A的逆矩阵 即B为 $A^-1$\n",
    "- 特征值 -- Ax = λx , 其中 A是矩阵, x是列向量, λ是实数, 则λ是特征值\n",
    "- 特征向量 -- -- Ax = λx , 其中 A是矩阵, x是列向量, λ是实数, 则x是特征向量\n",
    "- fully-connected layer -- 全连接层 它的每一个输入都通过矩阵-向量乘法得到它的每个输出\n",
    "- one-hot encoding -- 独热编码 独热编码是一个向量，它的分量(元素)和类别一样多, 类别对应的分量设置为1，其他所有分量设置为0\n",
    "- affine function - 仿射函数\n",
    "- bias-variance tradeoff -- 偏差-方差权衡\n",
    "- backpropagation 或 backward propagation -- 反向传播, 指的是计算神经网络参数梯度的方法, 按相反的顺序从输出层到输入层遍历网络, 该算法存储了计算某些参数梯度时所需的任何中间变量。\n",
    "- covariate shift -- 协变量偏移, 假设：虽然`输入的分布`可能随时间而改变， 但`标签函数`(即条件分布\n",
    "P(y|x))没有改变。 统计学家称之为协变量偏移\n",
    "- label shift -- 标签偏移, 描述了与协变量偏移相反的问题, 假设`标签边缘概率`P(y)可以改变， 但是`类别条件分布`P(x|y)在不同的领域之间保持不变。 当我们认为y导致x时，标签偏移是一个合理的假设\n",
    "- concept shift -- 概念偏移, 当`标签的定义`发生变化时，就会出现这种问题\n",
    "- nonstationary distribution -- 非平稳分布, 当分布变化缓慢并且模型没有得到充分更新时, 会出现更微妙的情况 非平稳分布\n",
    "- empirical risk -- 经验风险\n",
    "- true risk -- 真实风险\n",
    "- logistic regression -- 对数几率回归\n",
    "- batch learning  -- 批量学习\n",
    "- 单一神经网络\n",
    "    - 接受一些输入\n",
    "    - 生成相应的标量输出\n",
    "    - 具有一组相关 参数（parameters），更新这些参数可以优化某目标函数。\n",
    "- Laplace smoothing -- 拉普拉斯平滑 在所有计数中添加一个小常量\n",
    "- Zipf’s law -- 齐普夫定律 $$n_i \\propto \\frac {1}{i^\\alpha}$$ $$logn_i = -\\alpha logi + c$$"
   ],
   "id": "990e5df243a754bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 特殊符号\n",
    "    - ∇"
   ],
   "id": "b36045f608920182"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bcda6fec42620230"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
