{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c25b6ee4a299e8c",
   "metadata": {},
   "source": [
    "- 1.1 入门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T02:03:18.996049Z",
     "start_time": "2025-09-09T02:03:18.991068Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "12\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from numpy.conftest import dtype\n",
    "\n",
    "# print(torch.__version__)\n",
    "\n",
    "x = torch.arange(12)\n",
    "print(x)\n",
    "print(x.shape) # 访问张量的形状\n",
    "print(x.numel()) # 访问张量的元素个数\n",
    "# new_x = x.reshape(3, 4) # reshape函数, 改变一个张量的形状而不改变元素数量和元素值\n",
    "# new_x = x.reshape(-1, 4) # 和上面效果一样\n",
    "new_x = x.reshape(3, -1) # 和上面效果一样\n",
    "print(new_x)\n",
    "print(new_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee7a83f0fed83b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:54:57.104319Z",
     "start_time": "2025-09-09T01:54:57.071297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 形状为 2, 3, 4, 所有元素都是0的张量\n",
    "torch.zeros(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8106febd61a52c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:55:16.658314Z",
     "start_time": "2025-09-09T01:55:16.643832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 形状为 2, 3, 4, 所有元素都是0的张量\n",
    "torch.ones(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d2e45ce124dc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:59:38.478645Z",
     "start_time": "2025-09-09T01:59:38.471985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8897,  0.0402, -0.3216,  0.2580],\n",
      "        [-0.6214, -0.3166,  0.6893, -0.0964],\n",
      "        [ 0.5494, -1.6570, -1.2573,  0.6570]])\n",
      "tensor(-3.9661)\n"
     ]
    }
   ],
   "source": [
    "# 每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样\n",
    "r_x = torch.randn(3, 4)\n",
    "print(r_x)\n",
    "# print(r_x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b430aebd558c27f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T02:02:24.423633Z",
     "start_time": "2025-09-09T02:02:24.408275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1\n",
    "\n",
    "torch.tensor([[0, 1, 2], [3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633b885c3a1f294",
   "metadata": {},
   "source": [
    "- 1.2 运算符\n",
    "```text\n",
    "  对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、*、/和**）都可以被升级为按元素运算 -- 不同形状会有广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71856ec696ef1510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:54:39.168786Z",
     "start_time": "2025-09-09T11:54:39.147424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  4.,  6., 10.])\n",
      "tensor([-1.,  0.,  2.,  6.])\n",
      "tensor([ 2.,  4.,  8., 16.])\n",
      "tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
      "tensor([ 1.,  4., 16., 64.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1., 2., 4., 8.])\n",
    "y = torch.tensor([2., 2., 2., 2.])\n",
    "\n",
    "# x 和 y 具有相同的形状, 对 x 、y 进行的运算, 都可以升级为按元素运算\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print(x ** y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c4defd3755b46",
   "metadata": {},
   "source": [
    "- “按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d2bc5f5379dc9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T12:03:49.631693Z",
     "start_time": "2025-09-09T12:03:49.612069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1., 2., 4., 8.])\n",
    "print(torch.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c60886babec5c",
   "metadata": {},
   "source": [
    "- concatenate 函数将多个张量连接在一起, 可以指定按照哪个维度来连接, 比如: dim=0 表示在0维上连接(按行连接), dim=1 表示在1维上连接(按列连接) -- torch.cat 在指定维度上连接张量时，要求除连接维度外的其他维度大小必须完全一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50936cc1f68fa913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T12:32:48.578155Z",
     "start_time": "2025-09-09T12:32:48.551154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 4 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m X_2 = torch.arange(\u001B[32m12\u001B[39m).reshape(\u001B[32m3\u001B[39m, \u001B[32m4\u001B[39m)\n\u001B[32m      9\u001B[39m Y_2 = torch.arange(\u001B[32m12\u001B[39m).reshape(\u001B[32m4\u001B[39m, \u001B[32m3\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_2\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 4 but got size 3 for tensor number 1 in the list\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Sizes of tensors must match except in dimension 0. Expected size 4 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(torch.cat((X, Y), dim=0))\n",
    "print(torch.cat((X, Y), dim=1))\n",
    "\n",
    "\n",
    "X_2 = torch.arange(12).reshape(3, 4)\n",
    "Y_2 = torch.arange(12).reshape(4, 3)\n",
    "print(torch.cat((X_2, Y_2), dim=0))  # RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 4 but got size 3 for tensor number 1 in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c840be9a761a4",
   "metadata": {},
   "source": [
    "- 通过逻辑运算符构建二元张量 如果X和Y在该位置相等，则新张量中相应项的值为1, 意味着逻辑语句X == Y在该位置处为真，否则该位置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5038c89b14735f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:28:14.075048Z",
     "start_time": "2025-09-09T13:28:14.069216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "tensor([[ True, False,  True, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "tensor([[False, False, False, False],\n",
      "        [ True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(X == Y)\n",
    "print(X < Y)\n",
    "print(X > Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e084a1942ea5",
   "metadata": {},
   "source": [
    "- 求和, 对张量中的所有元素进行求和，会产生一个单元素张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b28283a5963c397f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T12:18:14.592486Z",
     "start_time": "2025-09-09T12:18:14.579150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "print(torch.sum(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f1d762c1db598",
   "metadata": {},
   "source": [
    "- 1.3 广播机制\n",
    "```text\n",
    "1.2节举的例子, 要求两个张量X和Y具有相同的形状。\n",
    "但是在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作\n",
    "\n",
    "1) 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n",
    "2) 对生成的数组执行按元素操作\n",
    "3) 一般情况下沿着数组中长度为1的轴进行广播 (通过下面的例子可以观察到, 需要广播的数组长度为1, 如果不是, 会报错)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8fbee3b8a70b8f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T14:19:55.477073Z",
     "start_time": "2025-09-09T14:19:55.472074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([[0, 1]])\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# a = torch.arange(6).reshape(3, 2)\n",
    "a = torch.arange(3).reshape(3, 1)\n",
    "print(a)\n",
    "b = torch.arange(2).reshape(1, 2)\n",
    "# b = torch.tensor([[0, 1], [2, 3]])\n",
    "print(b)\n",
    "print(\"*\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e022c8f2dd4ea343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T14:19:58.144032Z",
     "start_time": "2025-09-09T14:19:58.140607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(a + b)\n",
    "# print(torch.cat((a, b), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afdf7f7dd686aab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:30:44.934098Z",
     "start_time": "2025-09-09T13:30:44.929040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1],\n",
      "         [2, 3],\n",
      "         [4, 5]]])\n",
      "tensor([[0, 1]])\n",
      "tensor([[[0, 2],\n",
      "         [2, 4],\n",
      "         [4, 6]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(6).reshape(1, 3, 2)\n",
    "print(a)\n",
    "b = torch.arange(2).reshape(1, 2)\n",
    "# b = torch.tensor([[0, 1], [2, 3]])\n",
    "print(b)\n",
    "print( a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad6ce02162e406",
   "metadata": {},
   "source": [
    "- 1.4. 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f784bcb928337a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:01:24.279269Z",
     "start_time": "2025-09-09T13:01:24.269286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "\n",
    "print(X)\n",
    "print(X[-1])\n",
    "print(X[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e8a1470a9b7e0",
   "metadata": {},
   "source": [
    "- 除读取外，我们还可以通过指定索引来将元素写入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf649d96cd290d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:02:44.412196Z",
     "start_time": "2025-09-09T13:02:44.404021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  9.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "X[1, 2] = 9 # 将 2行 3列的数改为 9\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b20c6a99c3398f",
   "metadata": {},
   "source": [
    "- 为多个元素赋值相同的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ae0c76f8da6d4c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:05:32.229879Z",
     "start_time": "2025-09-09T13:05:32.225300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "X[0:2, :] = 12 # 将 0行 1行 的所有数改为 12\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedced22cc779d3",
   "metadata": {},
   "source": [
    "- 1.5. 节省内存\n",
    "```text\n",
    "运行一些操作可能会导致为新结果分配内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da25dd82690f9165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:17:16.691392Z",
     "start_time": "2025-09-09T13:17:16.686384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3038849999936\n",
      "3038849999936\n",
      "True\n",
      "--------------------\n",
      "id(Z): 3038849998416\n",
      "id(Z): 3038849998416\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "before = id(Y)\n",
    "Y = Y + X\n",
    "after = id(Y)\n",
    "\n",
    "print(before)\n",
    "print(after)\n",
    "print(before == after)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = Y + X\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39011ad0d18025",
   "metadata": {},
   "source": [
    "- 1.6. 转换为其他Python对象\n",
    "```text\n",
    "将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9c278fd0e99d088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:24:24.422173Z",
     "start_time": "2025-09-09T13:24:24.413885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "[[10.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]]\n",
      "tensor([[10.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "**************************************************\n",
      "[[20.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]]\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[20.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "A = X.numpy()\n",
    "# print(A)\n",
    "\n",
    "B = torch.tensor(A)\n",
    "print(B)\n",
    "\n",
    "print(type(A))\n",
    "print(type(B))\n",
    "\n",
    "# 张量 和 numpy 数之间进行转换, 底层公用了内存, 改变一个张量, 会同时改变另一个张量\n",
    "X[0, 0] = 10 # A 由 X.numpy 生成, 改变 X, 会同时改变 A\n",
    "print(A)\n",
    "print(X)\n",
    "print(B) # 不会改变\n",
    "\n",
    "print(\"*\" * 50)\n",
    "\n",
    "\n",
    "A[0, 0] = 20\n",
    "print(A)\n",
    "print(B) # 不会改变\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d9f4aa6751c62",
   "metadata": {},
   "source": [
    "- 将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ece4dbd247a65de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:26:54.337066Z",
     "start_time": "2025-09-09T13:26:54.329838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000]) 3.5 3.5 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([3.5])\n",
    "print(a , a.item(), float(a), int(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bdfd0d90163a6b",
   "metadata": {},
   "source": [
    "- 2. 数据预处理 -- 读取数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cbf1911d609b142d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:45:51.645264Z",
     "start_time": "2025-09-09T13:45:51.640263Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 创建文件夹\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n') # 列名\n",
    "    f.write('NA,Pave,127500\\n') # 每行表示一个数据样本\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363d1dc2917a456",
   "metadata": {},
   "source": [
    "- 导入pandas包并调用read_csv函数。从创建的CSV文件中加载原始数据集, 该数据集有四行三列, 其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b265581481f908e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:49:25.486294Z",
     "start_time": "2025-09-09T13:49:25.477068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c811e7281ae705",
   "metadata": {},
   "source": [
    "- 2.2. 处理缺失值\n",
    "```text\n",
    "注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括插值法和删除法，\n",
    "- 插值法: 用一个替代值弥补缺失值\n",
    "- 删除法: 则直接忽略缺失值"
   ]
  },
  {
   "cell_type": "code",
   "id": "5bc4d68f2b4e22e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T05:56:40.575949Z",
     "start_time": "2025-09-13T05:56:40.567941Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "#  通过位置索引iloc，我们将data分成inputs和outputs， 其中前者为data的前两列，而后者为data的最后一列。 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项\n",
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "# print(inputs)\n",
    "# print(outputs)\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True) # 显示 nan 数据列, dummy_na 用于控制是否为缺失值 NaN 创建虚拟变量 False 表示不处理缺失值 NaN  True 表示处理缺失值 NaN\n",
    "# inputs = pd.get_dummies(inputs)\n",
    "# print(inputs)\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)\n",
    "# print(type(inputs)) # <class 'pandas.core.frame.DataFrame'>\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave\n",
      "0       3.0        True\n",
      "1       2.0       False\n",
      "2       4.0       False\n",
      "3       3.0       False\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7665c7be79930dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T14:22:05.787225Z",
     "start_time": "2025-09-09T14:22:05.782492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 1., 0.],\n",
      "        [2., 0., 1.],\n",
      "        [4., 0., 1.],\n",
      "        [3., 0., 1.]], dtype=torch.float64) tensor([127500., 106000., 178100., 140000.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = inputs.to_numpy(dtype=float)\n",
    "x = torch.tensor(x)\n",
    "# print(x) # 4 * 3\n",
    "y = outputs.to_numpy(dtype=float)\n",
    "y = torch.tensor(y)\n",
    "# print(y) # 1 * 4\n",
    "\n",
    "print(x, y) # 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049c09bb198406d",
   "metadata": {},
   "source": [
    "- 2.5. 练习\n",
    "    - 删除缺失值最多的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e2a770c1f8a4ea60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T14:52:47.299931Z",
     "start_time": "2025-09-09T14:52:47.290653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms   Price\n",
      "0       3.0  127500\n",
      "1       2.0  106000\n",
      "2       4.0  178100\n",
      "3       3.0  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# miss_columns = data.isnull().sum() # 统计缺失值的数量\n",
    "miss_columns = data.isna().sum() # isnull 是 isna 的别名\n",
    "# print(miss_columns) # 统计列缺失值数量\n",
    "# print(miss_columns.idxmax())\n",
    "# print(type(miss_columns)) # <class 'pandas.core.series.Series'>\n",
    "\n",
    "# 删除 NaN 最多的列\n",
    "'''\n",
    "inplace 作用\n",
    "False, 对数据 data 做copy\n",
    "True, 直接在 data 的基础上错操作, 返回None\n",
    "'''\n",
    "# data_cleaned = data.drop(miss_columns.idxmax(), axis=1, inplace=False)\n",
    "data_cleaned = data.drop(columns=[miss_columns.idxmax()], axis=1, inplace=False)\n",
    "# print(data)\n",
    "# print(data_cleaned)\n",
    "\n",
    "numeric_columns = data_cleaned.select_dtypes(include=['number']).columns # 找到包含数字的列明\n",
    "# print(numeric_columns)\n",
    "# pandas 的正常行为：fillna() 只会替换 NaN 值，不会修改已有的有效数据\n",
    "data_cleaned[numeric_columns] = data_cleaned[numeric_columns].fillna(data_cleaned[numeric_columns].mean())\n",
    "# print(type(data_cleaned)) # <class 'pandas.core.frame.DataFrame'>\n",
    "print(data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6507ee4986dbc",
   "metadata": {},
   "source": [
    "    - 将预处理后的数据集转换为张量格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bb8c094959b2d1e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:04:10.279661Z",
     "start_time": "2025-09-09T15:04:10.274413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0000e+00, 1.2750e+05],\n",
      "        [2.0000e+00, 1.0600e+05],\n",
      "        [4.0000e+00, 1.7810e+05],\n",
      "        [3.0000e+00, 1.4000e+05]], dtype=torch.float64)\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# 在上面处理的结果上, 继续做这个需求\n",
    "\n",
    "data_cleaned_numpy = data_cleaned.to_numpy(dtype=float)\n",
    "# print(data_cleaned_numpy)\n",
    "data_cleaned_torch = torch.tensor(data_cleaned_numpy)\n",
    "print(data_cleaned_torch)\n",
    "print(data_cleaned_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581269c6375cc7a2",
   "metadata": {},
   "source": [
    "- 3. 线性代数\n",
    "    - 3.1. 标量\n",
    "    ```text\n",
    "不随空间、形状变化而改变,\n",
    "张量表示方式: 标量由只有一个元素的张量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c26eb81efa594ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:38:54.989078Z",
     "start_time": "2025-09-09T15:38:54.980992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.) tensor(1.) tensor(6.) tensor(1.5000) tensor(9.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "print(x + y, x - y, x * y, x / y, x ** y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe117e9c1633f4",
   "metadata": {},
   "source": [
    "- 3.2. 向量\n",
    "```text\n",
    "向量可以被视为标量值组成的列表。 这些标量值被称为向量的元素（element）或分量（component）\n",
    "通过一维张量表示向量\n",
    "单个向量的默认方向是列向量\n",
    "大量文献认为列向量是向量的默认方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ac9adef23d98c86e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:58:21.893236Z",
     "start_time": "2025-09-09T15:58:21.885397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# x = torch.arange(4)\n",
    "x = torch.tensor([2,2,2,2])\n",
    "print(x)\n",
    "print(x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764ac765d45a794",
   "metadata": {},
   "source": [
    "- 长度、维度和形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "724e20ba4d1a6972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:59:02.403985Z",
     "start_time": "2025-09-09T15:59:02.400158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([4])\n",
      "******************************\n",
      "3\n",
      "torch.Size([3, 4])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "向量场景下 或 一维数组张量场景下\n",
    "张量的某个轴的维数就是这个轴的长度。 len() 、shape\n",
    "'''\n",
    "# 长度 元素的数量\n",
    "print(len(x))\n",
    "print(x.shape)\n",
    "print(x.ndim)\n",
    "\n",
    "print('*' * 30)\n",
    "# 下面示例代码 x2 是矩阵, 不是向量\n",
    "x2 = torch.arange(12).reshape(3, 4)\n",
    "print(len(x2)) # 3 axis = 0 对应的行数\n",
    "# print(x2.t()) # 转置\n",
    "print(x2.shape)\n",
    "print(x2.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25098d3f61f0872",
   "metadata": {},
   "source": [
    "- 3.3. 矩阵\n",
    "```text\n",
    "正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "118945ec627ac787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:04:12.104623Z",
     "start_time": "2025-09-09T16:04:12.099750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5942916aafdc77",
   "metadata": {},
   "source": [
    "- 矩阵转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fff946411c2b5c8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:14:44.628337Z",
     "start_time": "2025-09-09T16:14:44.624404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  4,  8, 12, 16],\n",
      "        [ 1,  5,  9, 13, 17],\n",
      "        [ 2,  6, 10, 14, 18],\n",
      "        [ 3,  7, 11, 15, 19]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "交换矩阵的行和列时，结果称为矩阵的转置（transpose）, a.t() 来获取转置\n",
    "'''\n",
    "\n",
    "print(A.T)\n",
    "# print(A.t())\n",
    "# print(A.transpose(0, 1))\n",
    "# print(torch.transpose(A, 0, 1)) # 交换 ndim0、 ndim1 的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19247f8f76416d43",
   "metadata": {},
   "source": [
    "- 对称矩阵（symmetric matrix） -- 矩阵的转置等于自身, 方阵的一种特殊矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e2bc29dc9543aa47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:15:48.686290Z",
     "start_time": "2025-09-09T16:15:48.680253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2c494fd6d32aef7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:16:02.476486Z",
     "start_time": "2025-09-09T16:16:02.471120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a4b84ee5c11a7",
   "metadata": {},
   "source": [
    "- 3.4. 张量\n",
    "```text\n",
    "张量是描述具有任意数量轴的n维数组的通用方法\n",
    "- 向量是一阶张量\n",
    "- 矩阵是二阶张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a63e5a24f52e598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:25:47.100474Z",
     "start_time": "2025-09-09T16:25:47.094427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c607b74c3d6ae10",
   "metadata": {},
   "source": [
    "- 3.5. 张量算法的基本性质\n",
    "```text\n",
    "标量、向量、矩阵和任意数量轴的张量有一些实用的属性\n",
    "1. 任何按元素的一元运算都不会改变其操作数的形状\n",
    "2. 给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量 -- 可以参考第二节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9135800c5f650525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:32:04.470209Z",
     "start_time": "2025-09-09T16:32:04.464923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "tensor([[ 0,  2,  4,  6],\n",
      "        [ 8, 10, 12, 14],\n",
      "        [16, 18, 20, 22],\n",
      "        [24, 26, 28, 30],\n",
      "        [32, 34, 36, 38]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "B = A.clone()\n",
    "print(A)\n",
    "# print(B)\n",
    "print(A + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c8069bb44e80fe67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:34:47.884183Z",
     "start_time": "2025-09-09T16:34:47.878204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   1,   4,   9],\n",
      "        [ 16,  25,  36,  49],\n",
      "        [ 64,  81, 100, 121],\n",
      "        [144, 169, 196, 225],\n",
      "        [256, 289, 324, 361]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "两个矩阵的按元素乘法称为Hadamard积（Hadamard product）\n",
    "'''\n",
    "# 矩阵A 和 矩阵B Hadamard积\n",
    "print(A * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "705ae6030ee80167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:36:51.378128Z",
     "start_time": "2025-09-09T16:36:51.373128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[ 2,  3,  4,  5],\n",
      "         [ 6,  7,  8,  9],\n",
      "         [10, 11, 12, 13]],\n",
      "\n",
      "        [[14, 15, 16, 17],\n",
      "         [18, 19, 20, 21],\n",
      "         [22, 23, 24, 25]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘\n",
    "'''\n",
    "import torch\n",
    "\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "print(X)\n",
    "print(a + X)\n",
    "print((a * X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aed7e1f5ca6792",
   "metadata": {},
   "source": [
    "- 3.6. 降维\n",
    "```text\n",
    "可以对任意张量进行的一个有用的操作是计算其元素的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a4ff25882177b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:08:42.160208Z",
     "start_time": "2025-09-10T01:08:42.130848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3]) tensor(6)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "#\n",
    "# x = np.arange(4)\n",
    "# print(x, x.sum())\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "print(x, x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb1086cc33119e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:10:22.842024Z",
     "start_time": "2025-09-10T01:10:22.807711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4]) tensor(190.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "可以表示任意形状张量的元素和\n",
    "'''\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "print(A.shape, A.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "371f64c700893a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:13:53.805532Z",
     "start_time": "2025-09-10T01:13:53.800669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([40., 45., 50., 55.]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "A_sum_axis0 = A.sum(axis=0) # 列求和\n",
    "print(A, A_sum_axis0, A_sum_axis0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19430311250c6d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:15:45.898109Z",
     "start_time": "2025-09-10T01:15:45.893616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([ 6., 22., 38., 54., 70.]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "A_sum_axis1 = A.sum(axis=1) # 行求和\n",
    "print(A, A_sum_axis1, A_sum_axis1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2270a1b71760b67e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:21:31.755558Z",
     "start_time": "2025-09-10T01:21:31.747440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor(190.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "沿着行、列求和, 等价于对所有元素求和\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "# A_aum_all = A.sum(axis=[0, 1])\n",
    "A_sum_all = torch.sum(A, (0, 1))\n",
    "print(A, A_sum_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d542f8a23eb0535f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:24:33.115142Z",
     "start_time": "2025-09-10T01:24:33.108155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor(9.5000) torch.Size([]) tensor(9.5000)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "与求和相关的量是平均值（mean或average）\n",
    "'''\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "print(A, A.mean(), A.mean().shape, A.sum()/A.numel()) # A.mean().shape 没有维度的标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "220de9a8548711b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:29:00.860965Z",
     "start_time": "2025-09-10T01:29:00.854966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([ 8.,  9., 10., 11.]) tensor([ 8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "计算平均值的函数也可以沿指定轴降低张量的维度\n",
    "'''\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "print(A, A.mean(axis=0), A.sum(axis=0)/A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f7935e021c11b",
   "metadata": {},
   "source": [
    "- 3.6.1. 非降维求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d89003f77ff69ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:36:25.263165Z",
     "start_time": "2025-09-10T01:36:25.253707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor(190.) tensor([40., 45., 50., 55.])\n",
      "tensor([[40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "在调用函数来计算总和或均值时保持轴数不变会很有用\n",
    "'''\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "\n",
    "print(A, A.sum(), A.sum(axis=0))\n",
    "A_sum_axis0 = A.sum(axis=0, keepdim=True)\n",
    "print(A_sum_axis0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30dfa14905fb667b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:36:26.756522Z",
     "start_time": "2025-09-10T01:36:26.751213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor(190.) tensor([ 6., 22., 38., 54., 70.])\n",
      "tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "\n",
    "print(A, A.sum(), A.sum(axis=1))\n",
    "A_sum_axis1 = A.sum(axis=1, keepdim=True) # keepdim 保持张量的维度\n",
    "print(A_sum_axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cd8276695774aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:39:34.595730Z",
     "start_time": "2025-09-10T01:39:34.589637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]]) tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
      "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
      "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
      "        [0.2286, 0.2429, 0.2571, 0.2714]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "在上面步骤下, A.shape = (5,4)\n",
    "A_sum_axis1.shape = (5,1)\n",
    "A 和 A_sum_axis1 进行运算时可以通过广播的方式进行处理\n",
    "'''\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "A_sum_axis1 = A.sum(axis=1, keepdim=True) # keepdim 保持张量的维度\n",
    "print(A, A_sum_axis1, A/A_sum_axis1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c1586c346d31e450"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "552ffb8495b27ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:48:23.281429Z",
     "start_time": "2025-09-10T01:48:23.275926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([[ 0.,  1.,  3.,  6.],\n",
      "        [ 4.,  9., 15., 22.],\n",
      "        [ 8., 17., 27., 38.],\n",
      "        [12., 25., 39., 54.],\n",
      "        [16., 33., 51., 70.]]) tensor([ 6., 22., 38., 54., 70.])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "累加求和 想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），可以调用cumsum函数。 此函数不会沿任何轴降低输入张量的维度\n",
    "'''\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "# print(A, A.cumsum(axis=0), A.sum(axis=0))\n",
    "print(A, A.cumsum(axis=1), A.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544fd83f9641abff",
   "metadata": {},
   "source": [
    "- 3.7. 点积（Dot Product）\n",
    "```text\n",
    "点积: 相同位置的按元素乘积的和\n",
    "torch.dot 执行的是向量点积（也称为内积或标量积）, 是对应元素相乘后求和, 比如下面示例 # 0*1 + 1*1 + 2*1 + 3*1 = 0 + 1 + 2 + 3 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70e0c78ea2d19265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:10:12.157352Z",
     "start_time": "2025-09-10T02:10:12.149843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.]) tensor([1., 1., 1., 1.]) tensor(6.)\n",
      "tensor([0., 1., 2., 3.]) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "\n",
    "print(x, y, torch.dot(x, y))\n",
    "\n",
    "# 可以通过执行按元素乘法，然后进行求和来表示两个向量的点积：\n",
    "print(x * y, torch.sum(x * y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c124e6e8578b36",
   "metadata": {},
   "source": [
    "- 3.8. 矩阵-向量积\n",
    "\n",
    "矩阵向量积Ax是一个长度为m的列向量， 其第i个元素是点积 torch.dot(A(i), x)\n",
    "A = m * n 的矩阵\n",
    "x = 1 * n\n",
    "第i个元素是, A的第i行的 n 个列元素 和 x 进行点积的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "548ad3997d9b3d44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:20:56.535488Z",
     "start_time": "2025-09-10T02:20:56.524017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([0., 1., 2., 3.]) tensor([ 14.,  38.,  62.,  86., 110.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "\n",
    "print(A, x, torch.mv(A, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8812af6e2ec03",
   "metadata": {},
   "source": [
    "- 矩阵-矩阵乘法\n",
    "```text\n",
    "点积 -> 矩阵向量积 -> 矩阵-矩阵乘法\n",
    "矩阵-矩阵 乘法: AB, A=m*n, B=n*k  计算结果 C=m * k, C[i,j] = A[i,:] * B[:,j] 表示 A的第i行所有n个列元素 和 B的j列，所有的行元素继续点积的值\n",
    "C[i, :] 第i行所有元素 = A[i,:] * B[:, :] i行每列的元素 = A矩阵第i行的所有元素分别和b矩阵k列的值做 矩阵-向量积\n",
    "\n",
    "\n",
    "矩阵-矩阵乘法可以简单地称为矩阵乘法，不应与“Hadamard积”混淆\n",
    "- 矩阵乘法 torch.mm(A, B)\n",
    "- Hadamard积 A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca430fc409223665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:41:41.972239Z",
     "start_time": "2025-09-10T02:41:41.960302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) tensor([[ 6.,  6.,  6.],\n",
      "        [22., 22., 22.],\n",
      "        [38., 38., 38.],\n",
      "        [54., 54., 54.],\n",
      "        [70., 70., 70.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = torch.ones(4, 3)\n",
    "\n",
    "print(A, B, torch.mm(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6b6ccd1194cb7",
   "metadata": {},
   "source": [
    "- 3.10. 范数\n",
    "```text\n",
    "Ln范数, 所有元素的n次方之和再开n次方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef67ba5685af96b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T08:58:06.623056Z",
     "start_time": "2025-09-10T08:58:06.609743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 范数, 所有元素平方之和再开方\n",
    "import torch\n",
    "\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6f2c3fd6f629297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T08:59:08.460168Z",
     "start_time": "2025-09-10T08:59:08.446314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 范数, 所有元素绝对值之和\n",
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0f0f208f426dfe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:04:35.838783Z",
     "start_time": "2025-09-10T09:04:35.830783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# Frobenius 范数, 所有元素平方之和再开2次方\n",
    "print(torch.ones(4, 9))\n",
    "\n",
    "print(torch.norm(torch.ones(4, 9))) # L2 范数, 所有元素平方和之后, 再开2次方(平方根) = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03ddc77f8619b9",
   "metadata": {},
   "source": [
    "- 练习\n",
    "    - 证明一个矩阵A的转置的转置是A\n",
    "    - 给出两个矩阵A和B, 证明“它们转置的和”等于“它们和的转置”，\n",
    "    - 给定任意方阵A, A + A的转置 总是对称的吗?为什么?\n",
    "    - 本节中定义了形状(2,3,4)的张量X。len(X)的输出结果是什么\n",
    "    - 对于任意形状的张量X,len(X)是否总是对应于X特定轴的长度?这个轴是什么?\n",
    "    - 运行A/A.sum(axis=1)，看看会发生什么。请分析一下原因？\n",
    "    - 考虑一个具有形状(2,3,4)的张量，在轴0、1、2上的求和输出是什么形状?\n",
    "    - 为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2bb74ab590c5409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:12:38.655447Z",
     "start_time": "2025-09-10T09:12:38.647448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 证明一个矩阵A的转置的转置是A\n",
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "\n",
    "print(A == A.T.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc580379dfd23e",
   "metadata": {},
   "source": [
    "- 4. 微积分\n",
    "- 4.1. 导数和微分\n",
    "```text\n",
    "做下面的测试, 安装相关依赖\n",
    "1. d2l https://repo.huaweicloud.com/repository/pypi/simple/d2l/ 这里下载的是 d2l-0.17.6-py3-none-any.whl, 安装过程中可能会提示依赖版本问题, 需要将压缩文件中的 d2l-0.17.6.dist-info/METADATA 版本依赖 == 改为 >=\n",
    "2. torch、torchvision、torchaudio 因为使用的 torch 版本是2.4.1, 这里安装的 torchvision=0.19.1, torchaudio=2.4.1, 如果是其它版本, 可以网络查询下版本对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e36c851abac8d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T13:54:01.296728Z",
     "start_time": "2025-09-10T13:53:58.402792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "# 解释导数, 假设 u=f(x)=3x^2 - 4x\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 定义 f(x)\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 *x\n",
    "\n",
    "# 定义 f'(x)\n",
    "'''\n",
    "f 表示函数 f(x)\n",
    "x 表示变量取的值\n",
    "h 表示无限接近0的值\n",
    "'''\n",
    "def numerical_lim(f, x, h):\n",
    "    return (f(x+h) - f(x))/h\n",
    "\n",
    "\n",
    "h=0.1\n",
    "for i in range(5):\n",
    "    print(f'h={h:0.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
    "    h *= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b7b01d8f438bc",
   "metadata": {},
   "source": [
    "符号 d/dx 和 D 是微分运算符，表示微分操作。可以使用如下规则来对常见函数求微分\n",
    "- 常见规则\n",
    "    - DC = 0 C表示常量, 表示对常量求导\n",
    "    - Dx^n = nx^(n-1) n是任意实数, 表示对 x^n 求导\n",
    "    - De^x = e^x 表示对 e^x 求导\n",
    "    - Dlogx = 1/x 表示对 logx 求导\n",
    "- 常见法则\n",
    "    - 乘法法则 d[f(x)g(x])/dx = f(x)g'(x) + f'(x)g(x) = f(x)d[g(x)]/dx + g(x)d[f(x)]/dx\n",
    "    - 常数相乘法则 d|Cf(x)|/dx = Cf'(x)  其实可以这么理解: 常数求导为0 再结合乘法法则, 可以得到左边的式子\n",
    "    - 加法法则 d|f(x) + g(x)|/dx = f'(x) + g'(x)\n",
    "    - 除法法则 d[f(x)/g(x)]/dx = (g(x)f'(x) - f(x)g'(x))/g(x)^2 = (g(x)d|f(x)|/dx - f(x)d|g(x)|/dx)/g(x)^2\n",
    "\n",
    "\n",
    "- 导数, 可以理解为曲线 u=f(x) 在x点切线的斜率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d8ea6-d95d-4f91-821e-bdfcbc970313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注释 `#@save` 是一个特殊的标记，会将对应的函数、类或语句保存在d2l包中\n",
    "# 解释导数, 假设 u=f(x)=3x^2 - 4x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec22de34275a15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:30:16.576380600Z",
     "start_time": "2025-09-11T14:29:22.971835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 定义 f(x)\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 *x\n",
    "\n",
    "# 定义 f'(x)\n",
    "'''\n",
    "f 表示函数 f(x)\n",
    "x 表示变量取的值\n",
    "h 表示无限接近0的值\n",
    "'''\n",
    "def numerical_lim(f, x, h):\n",
    "    return (f(x+h) - f(x))/h\n",
    "\n",
    "\n",
    "h=0.1\n",
    "for i in range(5):\n",
    "    print(f'h={h:0.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
    "    h *= 0.1\n",
    "\n",
    "#@save\n",
    "def use_svg_display(): #\n",
    "    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "#@save\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"设置matplotlib的图表大小\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置matplotlib的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"定义一个plot函数来简洁地绘制多条曲线  绘制数据点\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca()\n",
    "\n",
    "    # 如果 x 有一个轴, 输出 True\n",
    "    def has_one_axis(X):\n",
    "        return  (hasattr(X,'ndim') and X.ndim == 1) or (isinstance(X, list) and not hasattr(X[0], '__len__'))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y) # 用这里的样例测试, len(Y)=2, X * len(Y) 表示X * 2 即复制2份\n",
    "    axes.cla()\n",
    "\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "\n",
    "\n",
    "# 绘制函数 u=f(x) 及其在 x=1 处的切线 y=2*x - 3\n",
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2*x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line(x=1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b94f04906f882a",
   "metadata": {},
   "source": [
    "- 4.2. 偏导数\n",
    "```text\n",
    "偏导数定义: 函数对某一变量的导数，保持其他变量不变\n",
    "\n",
    "上面只讨论了仅含一个变量的函数的微分。 在深度学习中，函数通常依赖于许多变量。 因此，我们需要将微分的思想推广到多元函数(multivariate function)上\n",
    "\n",
    "假设 y = f(x1, x2, x3,...,xn) 是一个具有n个变量的函数, y关于第i个参数xi的偏导数(partial derivative为)\n",
    "∂y/∂xi = lim (f(x1,x2,...,xi-1, xi+h, xi+1,...,xn) - f(x1,x2,...,xi-1, xi, xi+1,...,xn))/h (h->0 无限趋近与0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb137916ed3a48f",
   "metadata": {},
   "source": [
    "- 4.3. 梯度\n",
    "```text\n",
    "梯度(Gradient)：是一个向量, 由函数对所有变量的偏导数按顺序排列组成\n",
    "梯度是将多元函数对每个变量的偏导数“连结”成一个向量，该向量综合了函数在所有方向上的变化率，并指向函数值增长最快的方向\n",
    "\n",
    "设函数 f: R^n -> R 的输入是一个 n 维向量[x1,x2,...,xn]^T, 输出是一个标量。 函数f(x)相对于x的梯度是一个包含 n 个偏导数的向量 ∇f = [∂f/∂x1, ∂f/∂x2,...,∂f/∂xn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694730af6dddaf0",
   "metadata": {},
   "source": [
    "- 5. 自动微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c7fc4a1ea18fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T17:39:26.156354Z",
     "start_time": "2025-09-10T17:39:26.150324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "None\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor([True, True, True, True])\n",
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "x新梯度 tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "requires_grad_(True)：标记张量需要计算梯度\n",
    "y.backward()：执行反向传播，自动计算梯度\n",
    "x.grad：存储计算得到的梯度值\n",
    "x.grad.zero_()：清零梯度，避免累积\n",
    "\n",
    "xᵀx 表示点积（内积），结果是标量\n",
    "x×xᵀ 表示外积，结果是[4,4]矩阵\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "# 对函数 y = 2 * x^t * x 关于列向量求导\n",
    "x = torch.arange(4, dtype=torch.float32) # 注意: 这里可以理解为声明了 x0 ~ x3 的列向量, 只是现在取值分别是 0,1,2,3\n",
    "print(x)\n",
    "\n",
    "# requires_grad 是字段, requires_grad_ 是方法\n",
    "x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "print(x.grad) # 默认值 None\n",
    "\n",
    "# 计算点积\n",
    "y = 2 * torch.dot(x, x) # 0*0 + 1*1 + 2*2 + 3*3 = 0 + 1 + 4 + 9 = 14\n",
    "print(y)\n",
    "\n",
    "'''\n",
    "x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出。 接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度\n",
    "'''\n",
    "# 通过反向传播函数求梯度\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# y=2 * x^t * x 关于x的梯度是4\n",
    "print(x.grad == 4 * x)\n",
    "\n",
    "\n",
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "# print(x) # x 还是原来的值\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(\"x新梯度\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476d6c654498035f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T12:48:09.642504Z",
     "start_time": "2025-09-11T12:48:09.628490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4, dtype=torch.float32, requires_grad=True)\n",
    "print(x.grad)\n",
    "\n",
    "y = x * x # 对应元素平方\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8d55503edbc02f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:13:26.039004Z",
     "start_time": "2025-09-11T13:13:26.032039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4, dtype=torch.float32, requires_grad=True)\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "print(x.grad == u)\n",
    "\n",
    "\n",
    "x.grad.zero_() # 清空梯度缓存\n",
    "y.sum().backward()\n",
    "print(x.grad == 2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a01214520d37e",
   "metadata": {},
   "source": [
    "- 5.4. Python控制流的梯度计算\n",
    "```text\n",
    "使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fbeae0cd155bc5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:31:13.733654Z",
     "start_time": "2025-09-11T13:31:13.718504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1997, requires_grad=True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "'''\n",
    "返回一个张量, 这个张量填充的是来自正太分布的随机值, 这个正太分布的均值是0,方差是1 (也成为标准正太分布)\n",
    "'''\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "print(a)\n",
    "d = f(a)\n",
    "# print(d)\n",
    "d.backward()\n",
    "# d.backward() # 报错 RuntimeError: Trying to backward through the graph a second time\n",
    "# print(a.grad)\n",
    "# print(d/a)\n",
    "print(a.grad == d/a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c89b6c40f46fef96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:33:28.548402Z",
     "start_time": "2025-09-11T13:33:28.542401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor([   0.,  512., 1024., 1536.], grad_fn=<MulBackward0>)\n",
      "tensor([512., 512., 512., 512.])\n"
     ]
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.arange(4, dtype=torch.float32, requires_grad=True)\n",
    "print(a)\n",
    "d = f(a)\n",
    "print(d)\n",
    "d.sum().backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531e4bae21ff8a8",
   "metadata": {},
   "source": [
    "- 总结\n",
    "```text\n",
    "使用 深度学习框架计算指定变量梯度的思路\n",
    "1. 首先将梯度附加到想要对其计算偏导数的变量上\n",
    "2. 然后记录目标值的计算\n",
    "3. 执行它的反向传播函数\n",
    "4.并访问得到的梯度\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec26e42d758195",
   "metadata": {},
   "source": [
    "- 概率\n",
    "    - 基本概率论"
   ]
  },
  {
   "cell_type": "code",
   "id": "b94efc2a2deebf31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:39:56.904788Z",
     "start_time": "2025-09-11T14:39:56.897145Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.distributions import multinomial # 多项式\n",
    "from d2l import torch as d2l\n",
    "\n",
    "fair_probs = torch.ones([6])/6\n",
    "# print(multinomial.Multinomial(1, fair_probs).sample()) # 返回一个从fair_probs中抽取的样本\n",
    "# print(multinomial.Multinomial(100, fair_probs).sample()) # 返回100个从fair_probs中抽取的样本\n",
    "\n",
    "# counts = multinomial.Multinomial(10000, fair_probs).sample()\n",
    "# print(counts/10000)\n",
    "counts = multinomial.Multinomial(20, fair_probs).sample((500,))\n",
    "cum_counts = counts.cumsum(dim=0) # 做行累加, 当计算第i行时, 第i行结果 = 第i行的样本 + 第i-1行样本, 每行之和相加 = 20 * i\n",
    "print(cum_counts)\n",
    "estimates = cum_counts/cum_counts.sum(dim=1, keepdim=True)\n",
    "print(estimates)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2.,    4.,    3.,    2.,    3.,    6.],\n",
      "        [   9.,    7.,    4.,    6.,    6.,    8.],\n",
      "        [  13.,   10.,    5.,    9.,   11.,   12.],\n",
      "        ...,\n",
      "        [1675., 1660., 1633., 1668., 1678., 1646.],\n",
      "        [1679., 1662., 1634., 1671., 1683., 1651.],\n",
      "        [1679., 1666., 1634., 1679., 1688., 1654.]])\n",
      "tensor([[0.1000, 0.2000, 0.1500, 0.1000, 0.1500, 0.3000],\n",
      "        [0.2250, 0.1750, 0.1000, 0.1500, 0.1500, 0.2000],\n",
      "        [0.2167, 0.1667, 0.0833, 0.1500, 0.1833, 0.2000],\n",
      "        ...,\n",
      "        [0.1682, 0.1667, 0.1640, 0.1675, 0.1685, 0.1653],\n",
      "        [0.1682, 0.1665, 0.1637, 0.1674, 0.1686, 0.1654],\n",
      "        [0.1679, 0.1666, 0.1634, 0.1679, 0.1688, 0.1654]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c618c380b903d5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:36:10.862730300Z",
     "start_time": "2025-09-11T14:35:56.825020Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.distributions import multinomial # 多项式\n",
    "from d2l import torch as d2l\n",
    "\n",
    "fair_probs = torch.ones([6])/6\n",
    "\n",
    "counts = multinomial.Multinomial(10, fair_probs).sample((500,)) # 500组实验，每个样本有10个样本\n",
    "cum_counts = counts.cumsum(dim=0)\n",
    "print(cum_counts)\n",
    "estimates = cum_counts/cum_counts.sum(dim=1, keepdim=True)\n",
    "print( estimates)\n",
    "\n",
    "d2l.set_figsize((6, 4.5))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i].numpy(), label=(\"P(die=\" + str(i + 1) + \")\")) # [:, i] 表示取第i列\n",
    "\n",
    "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.,   1.,   0.,   1.,   3.,   3.],\n",
      "        [  4.,   1.,   0.,   4.,   5.,   6.],\n",
      "        [  6.,   3.,   0.,   7.,   6.,   8.],\n",
      "        ...,\n",
      "        [836., 838., 852., 823., 794., 837.],\n",
      "        [838., 840., 854., 824., 796., 838.],\n",
      "        [841., 843., 854., 825., 797., 840.]])\n",
      "tensor([[0.2000, 0.1000, 0.0000, 0.1000, 0.3000, 0.3000],\n",
      "        [0.2000, 0.0500, 0.0000, 0.2000, 0.2500, 0.3000],\n",
      "        [0.2000, 0.1000, 0.0000, 0.2333, 0.2000, 0.2667],\n",
      "        ...,\n",
      "        [0.1679, 0.1683, 0.1711, 0.1653, 0.1594, 0.1681],\n",
      "        [0.1679, 0.1683, 0.1711, 0.1651, 0.1595, 0.1679],\n",
      "        [0.1682, 0.1686, 0.1708, 0.1650, 0.1594, 0.1680]])\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 6.2. 处理多个随机变量",
   "id": "21ae60441eb95d5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a847a37d137a7480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7b8944cbe4417995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4389f1bc7b33a59b"
  },
  {
   "cell_type": "markdown",
   "id": "55deafa905463b58",
   "metadata": {},
   "source": [
    "- 查阅文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a2df9053faa7ac47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:47:17.910758Z",
     "start_time": "2025-09-09T16:47:17.906786Z"
    }
   },
   "outputs": [],
   "source": [
    "tuple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1ddb3ad7601c7553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:47:20.836612Z",
     "start_time": "2025-09-09T16:47:20.830786Z"
    }
   },
   "outputs": [],
   "source": [
    "list??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac31b7f3f56563b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:53:29.754511Z",
     "start_time": "2025-09-09T16:53:28.284539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'InverseGamma', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PositiveDefiniteTransform', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'inverse_gamma', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(dir(torch.distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69bc092d1be1e7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T16:55:37.275487Z",
     "start_time": "2025-09-09T16:55:37.270287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function transpose in module torch:\n",
      "\n",
      "transpose(...)\n",
      "    transpose(input, dim0, dim1) -> Tensor\n",
      "\n",
      "    Returns a tensor that is a transposed version of :attr:`input`.\n",
      "    The given dimensions :attr:`dim0` and :attr:`dim1` are swapped.\n",
      "\n",
      "    If :attr:`input` is a strided tensor then the resulting :attr:`out`\n",
      "    tensor shares its underlying storage with the :attr:`input` tensor, so\n",
      "    changing the content of one would change the content of the other.\n",
      "\n",
      "    If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` then the\n",
      "    resulting :attr:`out` tensor *does not* share the underlying storage\n",
      "    with the :attr:`input` tensor.\n",
      "\n",
      "    If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` with compressed\n",
      "    layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments\n",
      "    :attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must\n",
      "    both be sparse dimensions. The batch dimensions of a sparse tensor are the\n",
      "    dimensions preceding the sparse dimensions.\n",
      "\n",
      "    .. note::\n",
      "        Transpositions which interchange the sparse dimensions of a `SparseCSR`\n",
      "        or `SparseCSC` layout tensor will result in the layout changing between\n",
      "        the two options. Transposition of the sparse dimensions of a ` SparseBSR`\n",
      "        or `SparseBSC` layout tensor will likewise generate a result with the\n",
      "        opposite layout.\n",
      "\n",
      "\n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim0 (int): the first dimension to be transposed\n",
      "        dim1 (int): the second dimension to be transposed\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> x = torch.randn(2, 3)\n",
      "        >>> x\n",
      "        tensor([[ 1.0028, -0.9893,  0.5809],\n",
      "                [-0.1669,  0.7299,  0.4942]])\n",
      "        >>> torch.transpose(x, 0, 1)\n",
      "        tensor([[ 1.0028, -0.1669],\n",
      "                [-0.9893,  0.7299],\n",
      "                [ 0.5809,  0.4942]])\n",
      "\n",
      "    See also :func:`torch.t`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdd0feb13120eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
